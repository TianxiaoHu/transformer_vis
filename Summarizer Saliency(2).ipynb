{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saliency Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rinokeras.models.transformer import Transformer\n",
    "import tensorflow.contrib.eager as tfe\n",
    "import tensorflow.keras.backend as K\n",
    "from collections import namedtuple\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "from Dataset import Dataset\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tqdm, json\n",
    "import time\n",
    "import os\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "dataset = Dataset(\"summaries_testing.0.15.msgpack\")\n",
    "dataset.load()\n",
    "\n",
    "\n",
    "input_length = 400; output_length = 110\n",
    "n_layers = 10\n",
    "dim = 512\n",
    "d_filter = 2048\n",
    "n_vocab = len(dataset.vocab)\n",
    "\n",
    "model_file = \"summarize_10L_dataset_0.15_copy7_ckpt.h5\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-eager"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently a bit hacky, but will get the saliency matrix (dim = (110, 400)). Because the GPU cannot load the full graph into memory, we use batching below. Instructions for use:\n",
    "\n",
    "1. Run the following cell. \n",
    "2. Run the 2nd cell, starting with it = 0 (suggested schedule for it is [0, 20, 40, 60, 80] - on last iteration, set \"for j in range(30)\" and \"self.layer_activations.append([activations] * 30)\"). \n",
    "3. After running this 2nd cell, the graph has been built. Then run the following cell with everything uncommented as shown below. This generates the first 20 rows of the saliency map. On the final step, change to \"for j in range(30)\"\n",
    "4. Repeat steps 2 and 3, with the following changes: Update \"it\" to the next step in the above array (updating the lines above if it = 80). In step 3, comment out everything marked as \"TO BE COMMENTED\" before running, otherwise overwriting will happen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Summarizer():\n",
    "    def __init__(self, input_length, output_length, embedding, n_layers, it, d_filter=1216):\n",
    "        n_vocab, dim = embedding.shape\n",
    "\n",
    "        self.source_sequence = tf.placeholder(tf.int32,shape=(1,input_length), name=\"source_sequence\")\n",
    "        self.target_sequence = tf.placeholder(tf.int32, shape=(1,output_length),name=\"target_sequence\")\n",
    "        self.encoder_mask = tf.placeholder(tf.bool,shape=(1,input_length),name=\"encoder_mask\")\n",
    "        self.decoder_mask = tf.placeholder(tf.bool, shape=(1,output_length),name=\"decoder_mask\")\n",
    "\n",
    "        self.model = Transformer(discrete=True, n_symbols_in=n_vocab, d_model=dim, n_symbols_out=n_vocab, embedding_initializer=embedding, share_source_target_embedding=True, n_layers=n_layers, dropout=0.1, d_filter=d_filter)\n",
    "        print(\"Built the Transfomer\")\n",
    "        \n",
    "        # Train mode, get a loss, get your gradients\n",
    "        self.decoded_logits = self.model(self.source_sequence, self.target_sequence, encoder_mask=self.encoder_mask, decoder_mask=self.decoder_mask)\n",
    "\n",
    "        self.loss = tf.losses.sparse_softmax_cross_entropy(self.target_sequence, self.decoded_logits, tf.cast(self.decoder_mask, tf.float32), reduction = tf.losses.Reduction.NONE)\n",
    "        print(\"Built train mode + loss\")\n",
    "\n",
    "        encoding_stack = self.model.encoder.encoding_stack.layers\n",
    "\n",
    "        self.layer_activations = []\n",
    "        self.layer_gradients = []\n",
    "        for i in range(1):\n",
    "            layer_activate, layer_grad = [], []\n",
    "            activations = encoding_stack[0][i].my_output\n",
    "            for j in range(20):\n",
    "                gradients, = tf.gradients(self.loss[0][it+j], activations)\n",
    "                layer_grad.append(gradients)\n",
    "            self.layer_activations.append([activations] * 20)\n",
    "            self.layer_gradients.append(layer_grad)\n",
    "\n",
    "        print(\"Built encoder activations and gradients\")\n",
    "        \n",
    "        self.beam_decoded, _ = self.model.beam_decode(self.source_sequence, 110, beam_size=8, encoder_mask=self.encoder_mask)\n",
    "        print(\"Built beam-search decode.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built the Transfomer\n",
      "Built train mode + loss\n",
      "Built encoder activations and gradients\n",
      "Built beam-search decode.\n"
     ]
    }
   ],
   "source": [
    "it = 0\n",
    "embedding = np.random.randn(len(dataset.vocab), dim)\n",
    "model = Summarizer(input_length, output_length, embedding, n_layers, it, d_filter=d_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloaded previous\n",
      "(1, 110)\n",
      "(20, 400)\n"
     ]
    }
   ],
   "source": [
    "#TO BE COMMENTED\n",
    "inp, inp_mask, beam_decoded, first_beam, first_beam_mask  = [], [], [], [], []\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    model.model.load_weights(model_file)\n",
    "    print(\"Reloaded previous\")\n",
    "\n",
    "    #TO BE COMMENTED - SECTION BEGINS\n",
    "    inp, inp_mask = dataset.build_batch(['input', 'input_mask'], size=1)\n",
    "\n",
    "    inp = np.array(inp)\n",
    "    a = list(dataset.vocab[i] for i in inp[0])\n",
    "    print(a)\n",
    "    beam_decoded = sess.run(model.beam_decoded, {model.source_sequence: inp, model.encoder_mask: inp_mask})\n",
    "    print(beam_decoded)\n",
    "    first_beam = beam_decoded[:, 0, :]\n",
    "    b = list(dataset.vocab[i] for i in np.array(first_beam[0]))\n",
    "    print(b)\n",
    "    first_beam_mask = first_beam != dataset.vocab.index(\"<PAD>\")\n",
    "    print(first_beam)\n",
    "    print(first_beam_mask)\n",
    "\n",
    "    print(\"Beam decode summary:\")\n",
    "    print(dataset.evaluate_sentence(first_beam[0].tolist()))\n",
    "    #TO BE COMMENTED - SECTION ENDS\n",
    "\n",
    "    feed = {model.source_sequence: inp, model.target_sequence: first_beam, model.encoder_mask: inp_mask, model.decoder_mask: first_beam_mask}\n",
    "    all_vars = sess.run([model.loss] + model.layer_activations + model.layer_gradients, feed_dict=feed)\n",
    "\n",
    "    loss = all_vars[0]\n",
    "    activations = all_vars[1]\n",
    "    gradients = all_vars[2]\n",
    "\n",
    "    norms = []\n",
    "    for j in range(20):\n",
    "        temp_norm = np.sum(activations[j] * gradients[j], axis = 2)\n",
    "        norms.append(temp_norm)\n",
    "    norms = np.squeeze(np.array(norms))\n",
    "    print(norms.shape)\n",
    "    if os.path.isfile('saliency.npy'):\n",
    "        old_norms = np.load('saliency.npy')\n",
    "        norms = np.concatenate((old_norms, norms))\n",
    "    np.save('saliency.npy', norms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(110, 400)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = np.load('saliency.npy')\n",
    "A.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
